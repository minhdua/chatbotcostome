{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['áo cánh', 'Áo cánh', 'ao canh', 'Ao canh', 'áo sơ mi', 'Áo sơ mi', 'ao so mi', 'Ao so mi', 'áo khoác', 'Áo khoác', 'ao khoac', 'Ao khoac', 'áo len đan', 'Áo len đan', 'ao len dan', 'Ao len dan', 'quần áo ba lỗ', 'Quần áo ba lỗ', 'quan ao ba lo', 'Quan ao ba lo', 'áo ba lỗ', 'Áo ba lỗ', 'ao ba lo', 'Ao ba lo']\n"
     ]
    }
   ],
   "source": [
    "import pyvi\n",
    "from pyvi import ViUtils\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "\n",
    "#ViTokenizer.tokenize(u\"Trường đại học bách khoa hà nội\")\n",
    "#ViPosTagger.postagging(ViTokenizer.tokenize(u\"Trường đại học Bách Khoa Hà Nội\"))\n",
    "#from pyvi import ViUtils\n",
    "#ViUtils.remove_accents(u\"áo len tay ngắn\")\n",
    "#from pyvi import ViUtils\n",
    "#ViUtils.add_accents(u'ao len tay ngan')\n",
    "\n",
    "data = [\n",
    "    \"Áo cánh\",\n",
    "    \"Áo sơ mi\",\n",
    "    \"Áo khoác\",\n",
    "    \"Áo len đan\",\n",
    "    \"Quần áo ba lỗ\",\n",
    "    \"Áo ba lỗ\"\n",
    "]\n",
    "results = []\n",
    "for word in data:\n",
    "    text_remove_accents = ViUtils.remove_accents(word)\n",
    "    results.append(word.lower())\n",
    "    results.append(word.capitalize())\n",
    "    results.append(f'{text_remove_accents.lower()}')\n",
    "    results.append(f'{text_remove_accents.capitalize()}')\n",
    "    \n",
    "data_results = []\n",
    "for text in results:\n",
    "    data_results.append(text.replace(\"b'\", \"\").replace(\"'\", \"\"))\n",
    "    \n",
    "print(data_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "def simple_usage():\n",
    "    vncorenlp_file = r'backend/VnCoreNLP-1.2.jar'\n",
    "\n",
    "    sentences = 'VTV đồng ý chia sẻ bản quyền World Cup 2018 cho HTV để khai thác. ' \\\n",
    "                'Nhưng cả hai nhà đài đều phải chờ sự đồng ý của FIFA mới thực hiện được điều này.'\n",
    "\n",
    "    # Use only word segmentation\n",
    "    with VnCoreNLP(vncorenlp_file, annotators=\"wseg\") as vncorenlp:\n",
    "        print('Tokenizing:', vncorenlp.tokenize(sentences))\n",
    "\n",
    "    # Specify the maximum heap size\n",
    "    with VnCoreNLP(vncorenlp_file, annotators=\"wseg\", max_heap_size='-Xmx4g') as vncorenlp:\n",
    "        print('Tokenizing:', vncorenlp.tokenize(sentences))\n",
    "\n",
    "    # For debugging\n",
    "    with VnCoreNLP(vncorenlp_file, annotators=\"wseg\", max_heap_size='-Xmx4g', quiet=False) as vncorenlp:\n",
    "        print('Tokenizing:', vncorenlp.tokenize(sentences))\n",
    "\n",
    "\n",
    "simple_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Collect a corpus of Vietnamese text.\n",
    "with open('corpus/corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "    # Tokenize the text.\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "    # Create n-grams from the tokenized text.\n",
    "    bigrams = nltk.bigrams(tokens)\n",
    "    trigrams = nltk.trigrams(tokens)\n",
    "\n",
    "    # Count the frequency of each n-gram.\n",
    "    bigram_freq = nltk.FreqDist(bigrams)\n",
    "    trigram_freq = nltk.FreqDist(trigrams)\n",
    "\n",
    "\n",
    "    # Print the most common bigrams and trigrams.\n",
    "    print(bigram_freq.most_common(10))\n",
    "    print(trigram_freq.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
